## 方差

`Var(x) = (1 / (n - 1)) * Σ[i=1 to n] (xᵢ - x̄)²` 或 
`variance = sum((x_i - mean) ** 2 for x_i in x) / (len(x) - 1)`

用 NumPy 计算方差
```python
import numpy as np

n = np.array([1, 2, 3, 4, 5, 6])

# 计算样本方差（除以 n-1）
var_sample = np.var(n, ddof=1) #ddof=1就是除以n-1

# 计算总体方差（除以 n）
var_population = np.var(n, ddof=0)

print("样本方差:", var_sample)
print("总体方差:", var_population)
```

手动实现方差计算
```python
# 原始数据
data = [1, 2, 3, 4, 5, 6]

# 第一步：计算平均值
mean_value = sum(data) / len(data)

# 第二步：计算每个数减去平均值的平方，并加和
squared_diffs = 0
for x in data:
    diff = x - mean_value
    squared = diff ** 2
    squared_diffs += squared

# 第三步：除以 n-1 得到样本方差
variance = squared_diffs / (len(data) - 1)

print("样本方差 =", variance)
```

---
## 标准差
就是方差的平方根
```python
import numpy as np

data = [1, 2, 3, 4, 5, 6]

# 样本方差
var = np.var(data, ddof=1)

# 标准差 = 方差开根号
std = np.sqrt(var)

print("方差:", var)
print("标准差:", std)
```

用底层代码手动实现标准差计算

```python
# 原始数据
data = [1, 2, 3, 4, 5, 6]

# 第一步：计算平均值
mean_value = sum(data) / len(data)

# 第二步：计算每个数减去平均值的平方，并加和
squared_diffs = 0
for x in data:
    diff = x - mean_value
    squared = diff ** 2
    squared_diffs += squared

# 第三步：除以 n-1 得到样本方差
variance = squared_diffs / (len(data) - 1)

print("样本方差 =", variance)
# 第四步：标准差 = 方差的平方根
standard_deviation = variance ** 0.5

print("标准差 =", standard_deviation)

```
✅ 第五步：标准化每个数据点（z-score）
如果你想进一步做标准化处理（即 z-score），可以这样写：

```python
# 第五步：标准化每个数据点
standardized_data = []
for x in data:
    z = (x - mean_value) / standard_deviation
    standardized_data.append(z)

print("标准化后的数据 =", standardized_data)
```

---
## 标准误
简称 SE是衡量样本统计量的稳定性的指标，最常见的是 均值的标准误，表示样本均值对总体均值的估计误差,
（标准差）/[（样本数）**0.5]

✅ 用 NumPy 封装计算标准误
```python
import numpy as np

data = np.array([1, 2, 3, 4, 5, 6])

# 样本标准差（除以 n-1）
std = np.std(data, ddof=1)

# 样本容量
n = len(data)

# 标准误 = 标准差 / sqrt(n)
se = std / np.sqrt(n)

print("标准误 =", se)
```
✅ 如果你用 SciPy（更专业）
```python
from scipy import stats

se = stats.sem(data)  # sem = standard error of the mean
这个 stats.sem() 就是专门封装好的标准误函数，默认也是除以 n - 1。
```

✅ Python 底层实现（一句一句写）
```python
# 原始数据
data = [1, 2, 3, 4, 5, 6]

# 第一步：计算平均值
mean = sum(data) / len(data)

# 第二步：计算方差
squared_diff_sum = 0
for x in data:
    squared_diff_sum += (x - mean) ** 2
variance = squared_diff_sum / (len(data) - 1)

# 第三步：计算标准差
std_dev = variance ** 0.5

# 第四步：计算标准误
standard_error = std_dev / (len(data) ** 0.5)

print("标准误 =", standard_error)
```
---
## 样本方差和总体方差的区别
🎯 核心区别：除以 n 还是 n - 1  
✅ 为什么样本方差要除以 n - 1？
这是为了消除偏差（unbiased estimation）：

当你只拿到一个样本时，用样本均值代替总体均值，会低估真实的方差。

所以我们用 n - 1 来“补偿”这个低估，称为 贝塞尔校正（Bessel's correction）
|应用场景|推荐使用
|---|---
|描述整个数据集|总体方差
|从样本推断总体特征|样本方差
|生信差异分析|样本方差
|模型训练/标准化|样本方差

---
## 协方差（Covariance）
是衡量两个变量之间线性关系强度和方向的统计量。它是方差的推广：方差衡量一个变量的波动，协方差则衡量两个变量是否一起波动。
|协方差值|关系解释
|---|---
|> 0|X 和 Y 同时增加（正相关
|< 0|X 增加时 Y 减少（负相关）
|= 0|X 和 Y 无线性关系

✅ Python 实现（底层手写）
```python
# 两组数据
X = [1, 2, 3, 4, 5]
Y = [2, 4, 6, 8, 10]

# 计算均值
mean_X = sum(X) / len(X)
mean_Y = sum(Y) / len(Y)

# 计算协方差
cov = 0
for i in range(len(X)):
    cov += (X[i] - mean_X) * (Y[i] - mean_Y)

cov /= (len(X) - 1)

print("协方差 =", cov)
```
✅ 用 NumPy 快速计算
```python
import numpy as np

X = np.array([1, 2, 3, 4, 5])
Y = np.array([2, 4, 6, 8, 10])

cov_matrix = np.cov(X, Y)  # 返回协方差矩阵
print("协方差 =", cov_matrix[0, 1])
```

---
## 相关系数（Correlation Coefficient）
是衡量两个变量之间线性关系强度和方向的标准化指标。它是协方差的“标准化版本”，消除了单位影响，数值范围在 -1 到 1 之间。

|𝑟值范围|关系类型
|---|---
|𝑟=1|完全正相关
|𝑟=−1|完全负相关
|r =0|无线性相关
|r<0|趋势相反（负相关）
|r>0|趋势一致（正相关）

✅ Python 底层实现（一步一步写）
```python
X = [1, 2, 3, 4, 5]
Y = [2, 4, 6, 8, 10]

# 均值
mean_X = sum(X) / len(X)
mean_Y = sum(Y) / len(Y)

# 协方差
cov = 0
for i in range(len(X)):
    cov += (X[i] - mean_X) * (Y[i] - mean_Y)
cov /= (len(X) - 1)

# 标准差
std_X = (sum((x - mean_X) ** 2 for x in X) / (len(X) - 1)) ** 0.5
std_Y = (sum((y - mean_Y) ** 2 for y in Y) / (len(Y) - 1)) ** 0.5

# 相关系数
r = cov / (std_X * std_Y)

print("相关系数 =", r)
```
✅ 用 NumPy 快速计算
```python
import numpy as np

X = np.array([1, 2, 3, 4, 5])
Y = np.array([2, 4, 6, 8, 10])

r_matrix = np.corrcoef(X, Y)
print("相关系数 =", r_matrix[0, 1])
```
✅ 应用场景
基因共表达分析（WGCNA）

空间邻接图构建

特征选择与冗余检测

生信模块筛选（高相关基因聚类）

---
🧠 使用 scipy.stats 实现偏度和峰度
```python
from scipy.stats import skew, kurtosis

# 对每个基因（行）计算偏度和峰度
gene_skew = df.apply(skew, axis=1)
gene_kurt = df.apply(kurtosis, axis=1, fisher=False)  # 设置 fisher=False 得到真实峰度（不减3）

# 合并结果
df_stats = pd.DataFrame({
    'Skewness': gene_skew,
    'Kurtosis': gene_kurt
})
print(df_stats.head())
```

---
🧪 1. Shapiro-Wilk 检验（夏皮洛检验）
✅ 用途：
检验样本是否服从正态分布

📌 特点：
对小样本（n < 50）非常敏感，适合生物医学数据

原假设：数据服从正态分布

P 值小于 0.05 → 拒绝原假设 → 数据不服从正态分布

🧠 应用场景：
检查表达量、临床指标是否满足正态性，决定是否使用 t 检验或非参数检验

🧪 Python 示例：
```python
from scipy.stats import shapiro
stat, p = shapiro(data)
```

---
🧪 2. Kolmogorov-Smirnov 检验（KS检验）
✅ 用途：
检验样本是否来自某个理论分布（不仅限于正态分布）

📌 特点：
原假设：样本分布与指定分布一致

P 值小于 0.05 → 拒绝原假设 → 数据不符合指定分布

对尾部差异更敏感，但对小样本不如 Shapiro-Wilk 稳定

🧠 应用场景：
检查数据是否服从正态、均匀、指数等任意分布

可用于模型残差检验、拟合优度检验

🧪 Python 示例：
```python
from scipy.stats import kstest
stat, p = kstest(data, 'norm')  # 检验是否服从正态分布
```

---
🧠 什么是原假设？
原假设（H₀）通常是“没有差异”或“没有效应”的假设。例如：

两组表达量没有差异

某个治疗没有效果

某个基因与疾病无关

✅ P 值的意义
P 值是：在原假设成立的前提下，观察到当前数据或更极端结果的概率

所以当 P 值很小（如 < 0.05），就说明这种结果在“原假设为真”的情况下很难发生 → 我们倾向于拒绝原假设

📌 举个例子
你在做差异表达分析，比较处理组和对照组某个基因的表达：

原假设：处理组和对照组表达量没有差异

得到 P 值 = 0.01 → 表示这种差异在“无差异”的前提下只会有 1% 的概率发生

所以我们拒绝原假设，认为这个基因在两组之间有显著差异

---
在正态性检验中，如果 P 值大于 0.05，我们通常认为：

没有足够证据拒绝原假设 → 数据“可以认为”符合正态分布。

🧠 正态性检验的逻辑（以 Shapiro-Wilk 为例）
原假设 H₀：数据服从正态分布

备择假设 H₁：数据不服从正态分布

如果 P < 0.05 → 拒绝原假设 → 数据不服从正态分布

如果 P ≥ 0.05 → 保留原假设 → 数据可以认为服从正态分布

✅ 举个例子
```python
from scipy.stats import shapiro

data = [10.2, 9.8, 10.5, 10.1, 9.9]
stat, p = shapiro(data)
print(p)
```
如果输出 p = 0.12 → 数据可以认为是正态分布

如果输出 p = 0.01 → 数据不服从正态分布

⚠️ 注意事项
P 值大于 0.05 ≠ 数据“绝对”正态，只是“没有证据说它不是”

小样本时，检验功效低，可能漏检非正态

大样本时，微小偏差也可能导致 P < 0.05

所以，正态性检验只是参考，配合直方图、QQ图一起判断更稳妥

---
数据符合正态分布并不意味着差异基因分析无法做下去，反而是可以使用更强的参数检验方法。你问得非常好，这是很多人在统计学习中容易误解的地方。

✅ 正态分布是差异分析的“加分项”，不是“阻碍”
在差异表达分析中，我们通常关心的是：

两组样本之间是否有显著表达差异

是否可以使用 t 检验、ANOVA 等参数检验方法

而这些方法的前提之一就是：数据近似服从正态分布。

🧪 正态分布的意义
|检验方法	|是否需要正态性	|说明
|---|---|---
|t 检验（如 ttest_ind）|	✅ 需要|	如果数据正态，可以直接用
|limma（线性模型）|	✅ 假定正态残差	|更稳健，适合小样本
|非参数检验（如 Mann-Whitney）	|❌ 不需要	|用于非正态数据或小样本
|DESeq2 / edgeR|	❌ 不需要	|RNA-seq 原始计数数据，使用负二项分布建模

🧠 所以你的情况是：
如果你做的是 log2 转换后的表达数据分析（如 microarray 或 TPM/FPKM），符合正态分布 → 可以放心使用 t 检验或 limma。

📌 小结
|情况	|是否能做差异分析
|---|---
|数据符合正态分布	|✅ 可以做，甚至更稳健
|数据不符合正态分布	|✅ 仍然可以做，用非参数或建模方法

如果你做的是 原始计数数据分析（如 raw counts from RNA-seq），则不需要正态分布 → 用 DESeq2、edgeR 等建模方法。
